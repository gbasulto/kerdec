---
title: "Kernel Deconvolution Density Estimation"
author: "Guillermo Basulto-Elias"
date: "`r Sys.Date()`"
output: 
    rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r global_options, include=FALSE}
rm(list=ls()) ### To clear namespace
library(knitr)

# opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
#                echo=TRUE, warning=FALSE, message=FALSE)
opts_chunk$set(fig.width= 7, fig.height= 5,
               echo=TRUE, warning=FALSE, message=FALSE)


```


# Introduction

A well studied nonparametric estimator of probability density functions based on the i.i.d. observations $X_1, \ldots, X_n$, is the kernel density estimator. A natural generalization to the case where random variables have been contaminated with random additive noise, say $Y_1, \ldots, Y_n$, with $Y_i = X_i + \epsilon_i$, $i = 1, \ldots n$ is the so kernel deconvolution density estimatior (kdde). The error distribution is usually assumed to be known.

There are two popular `R` packages that provide functions to do this, `decon` and `deamer` (**REFERENCES**). The former package performs kdde with known error distribution allowing the user to pick the bandwidth selection method between several methods summarized in Delaigle and Gijbels (2004)(**REFERENCES**). The latter package covers the univariate case and it allows known error case, repeated observations and pure error case, the bandwidth selection method follows Comte (**2011???**)

The package `kerdec` provides functions to handle univariate and bivariate kdde with unknown error distribution, by using the empirical characteristic function, or knwon errors, which can be Laplace or Gaussian. Also functions are provided to estimate the parameters based on a sample of errors or panel data structured data.  

# Univariate kernel density deconvolution examples

```{r}
library(kerdec)                         # Load package

## Settings and samples for all the cases
n <- 150                                # Sample size
l <- 5                                  # Number of columns
m <- n + 10                             # Error sample size
shape <- 5                              # X distr. shape par.
rate <- 5                               # X distr. rate par.
sd_error <- .2                          # std. error of error distr.
X <- rgamma(n, shape, rate)             # Uncontaminated sample
eps_panel <- matrix(rlaplace(n*l, sd = sd_error),
                    n, l)               # Panel of errors
eps <- rlaplace(m, sd = sd_error)       # Pure errors
Y <- X + eps_panel[, 1]                 # Contaminated sample
Y_panel <- sweep(x = eps_panel, MARGIN = 1,
                 STATS = X, FUN = "+")  # Contaminated in panel


plot(function(x) dgamma(x, shape, rate), 0, 4)

## ------------------------------------------------------------
##           Case 1: Normal error known

select_bw(Y,
          method = c("CV", "NR")[1],
          kernel = c("sinc", "vp", "triw", "tric",
                     "flat")[5],
          h0 = NULL,
          error_dist = c("Normal", "Laplace", "None")[1],
          error_scale_par = NULL,
          error_smp = rnorm(1000, 0, 3^2),
          resolution = 128,
          error_proc = c("all", "vs_first",
                                     "indep_pairs")[1],
          panel_proc = c("keep_first", "take_aver")[1],
          truncation_bound = NULL)



```

# Density Estimation


It is desired to provide a nonparametric density estimator of a random vector $\boldsymbol{X}$ a random vector in $\mathbb{R}^d$ based on samples contaminated with additive error $\boldsymbol{\epsilon}$ say
$$\boldsymbol{Y} = \boldsymbol{X} + \boldsymbol{\epsilon}$$.


Let $\boldsymbol{X}_1, \ldots, \boldsymbol{X}_n$ be a sample of independent and identically distributed (_iid_) random vectors in $\mathbb{R}^d$. Suppose that it is desired to provide a density estimator 

where for $i = 1, \ldots, n$,
$$\boldsymbol{Y}_i = \boldsymbol{X}_i + \boldsymbol{\epsilon}_i.$$ 

Kernel deconvolution formula.

$$
\hat{f}_{\boldsymbol{X}}(\boldsymbol{x}) = 
    \frac{1}{(2\pi)^d}
    \int e^{-\imath \langle \boldsymbol{x}, \boldsymbol{t} \rangle}
    \hat{\phi}_{\boldsymbol{Y}, n}(\boldsymbol{t})
    \frac{K^{Ft}(H^{-1/2}\boldsymbol{t})}
         {\hat{\phi}_{\boldsymbol{\epsilon}}(\boldsymbol{t})}
    d\boldsymbol{t}
$$

We will consider two approaches to prodive bandwidth. The first of them consists on minimizing the AMISE by plugging in a normal reference estimator to R(f''). The second is my crossed-validation. We will only consider cases with one smoothing parameter.

```{r}
library(kerdec)

## Generate always the same results
set.seed(666)

## Set sample size and number of replicates
n <- 100
l <- 4
resolution <- 128

## Generate samples
                                        # Uncontaminated vector
X <- rgamma(n, shape = 5, rate = 5)     
                                        # Error
error_panel <- matrix(rnorm(n = n*l, mean = 0, sd = .2), ncol =  l)
error_vec <- error_panel[, 1]
                                        # Contaminated samples
Y_panel <- sweep(x = error_panel, MARGIN = 1, STATS = X, FUN = "+")
Y_vec <- X + error_vec


## Limits where function is going to be estimated and kernel
low <- -1
upp <- 6
ker <- 3

xx <- seq(low, upp, by = (upp - low)/resolution)[-(resolution + 1)]

yy <- Re(kerdec_dens_panel_1d_cpp(Y_panel, h = .09, low, upp,
                                  resolution, ker = ker))

zz <- Re(kerdec_dens_pure_1d_cpp(smp = Y_vec, error_smp = error_vec,
                              h = .09, low, upp, resolution, ker = ker))

uu <- dgamma(xx, shape = 5, rate = 5)

plot(range(xx), range(c(yy, zz, uu)), type = "n",
     ylab = "value", xlab = "x")
lines(xx, uu, type = "l", col = 2, lty = 1, lwd = 1.5)
lines(xx, yy, type = "l", col = 3, lty = 2, lwd = 1.5)
lines(xx, zz, type = "l", col = 4, lty = 3, 1.5)
legend("topright", legend = c("true", "panel", "pure"),
       col = 2:4, lty = 1:3)

```


## Kernel deconvolution density estimation on panel data

Information about the error distribution can be extracted from panel data by taking differences or observations for the same individual, which produces differences of errors. For example, if $Y_{ij_1} = X_i + \epsilon_{ij_1}$ and $Y_{ij_2} = X_i + \epsilon_{ij_2}$ are two observations with measurement error for the same individual, then $Y_{ij_1} - Y_{ij_2} = \epsilon_{ij_1} - \epsilon_{ij_2}$.

If there are only two observations per individual, this process is as simple as taking the difference between these two. However, when there are more than two observations per individual, there is not a unique way to do this. It is possible to take all possible differences, resulting on a sample of non-independent differences; to obtain independent samples, although fewer differences would be available, or a compromise between them.

The function `process_differences()` considers three approaches which are described below. For the examples, consider a panel data array of contaminated observations with $n$ individuals and every measurement taken $l$ times, that is, 
$$
\begin{matrix}
Y_{11} & \cdots & Y_{1k} \\
Y_{21} & \cdots & Y_{2k} \\
\vdots & \ddots & \vdots \\
Y_{n1} & \cdots & Y_{nk}
\end{matrix}.
$$


```{r}
library(kerdec)

aa <- kerdec_dens_panel_1d_cpp(matrix(rnorm(12), 3, 4), h = .5, 
                               lower = -2, upper = 2,
                               resolution = 64, ker = 1)

str(aa)

```



# Approximating Error


Consider the panel data 
$$
\begin{matrix}
Y_{11} & \cdots & Y_{1k} \\
Y_{21} & \cdots & Y_{2k} \\
\vdots & \ddots & \vdots \\
Y_{n1} & \cdots & Y_{nk}
\end{matrix},
$$
where $Y_{ij} = X_i + \epsilon_{ij}$. We want to approximate the characteristic function of the error, $\phi_{\epsilon}(\cdot)$ based on these data. 

Recall that $Y_{ij_1} - Y_{ij_2} = \epsilon_{ij_1} - \epsilon_{ij_2}$, that is, it is possible of extract differences of errors by taking differences of observations for the same individual. In vignette _differences_, we describe three possible ways to do this using the function `process_differences()`. Let $L$ denote the number of differences (thus, $L = nk(k - 1)/2$, $n(k - 1)$ or $n\lfloor k\rfloor/2$). Let $\delta_1, \ldots, \delta_L$ be such differences.

## Nonparametric approach

Assume that the error distribution is symmetric and its characteristic function is never zero. The latter condition is true is $\boldsymbol{\epsilon}$ is Laplace or normal, for instance. Then $\phi_{\boldsymbol{\delta}}(\boldsymbol{t}) = 
  \phi_{\boldsymbol{\epsilon}}^2(\boldsymbol{t})$

$$
\hat{\phi}_{\epsilon}(t) = 
\sqrt{\lvert \hat{\phi}_{\boldsymbol{\delta}} (\boldsymbol{t}) \rvert}
$$

Therefore, 
$$
\hat{\phi}_{\bar{\epsilon}}(t) =
\left[\hat{\phi}_{\epsilon}(t/k)\right]^k
$$

```{r, fig.width= 8, fig.height= 6}
library(kerdec)

set.seed(1810)

n <- 150
k <- 4
smp <- matrix(rnorm(n*k), n, k)
t <- seq(-10, 10, .05)

averaged_smp <- rowMeans(smp);

ecf1 <- ecf_mod(t, averaged_smp)
ecf2 <- error_cf_approx(t = t, smp = smp, diff_method = 1)
ecf3 <- exp(-t^2/2/k)

plot(range(t), range(c(ecf1, ecf2, ecf3)), type = "n", col = "green",
     ylab = "value", xlab = "t")
lines(t, ecf1, col = "green")
lines(t, ecf2, col = "blue")
lines(t, ecf3, col = "red")

```


# Differences


Information about the error distribution can be extracted from panel data by taking differences or observations for the same individual, which produces differences of errors. For example, if $Y_{ij_1} = X_i + \epsilon_{ij_1}$ and $Y_{ij_2} = X_i + \epsilon_{ij_2}$ are two observations with measurement error for the same individual, then $Y_{ij_1} - Y_{ij_2} = \epsilon_{ij_1} - \epsilon_{ij_2}$.

If there are only two observations per individual, this process is as simple as taking the difference between these two. However, when there are more than two observations per individual, there is not a unique way to do this. It is possible to take all possible differences, resulting on a sample of non-independent differences; to obtain independent samples, although fewer differences would be available, or a compromise between them.

The function `process_differences()` considers three approaches which are described below. For the examples, consider a panel data array of contaminated observations with $n$ individuals and every measurement taken $l$ times, that is, 
$$
\begin{matrix}
Y_{11} & \cdots & Y_{1k} \\
Y_{21} & \cdots & Y_{2k} \\
\vdots & \ddots & \vdots \\
Y_{n1} & \cdots & Y_{nk}
\end{matrix}.
$$

### Method 1: All pairwise differences

We consider all the differences $Y_{ij_1} - Y_{ij_2}$, for individual $i \in {1, \ldots, n}$ and $1 \leq j_2 < j_1 \leq l$. Therefore, it must be $nl(l-1)/2$ differences.

### Method 2: All versus first 



$$ 
\begin{bmatrix}
0 & 0 \\
1 & 1 
\end{bmatrix}
$$

```{r}

library(kerdec)

smp <- matrix(c(0, 1, 3, 6, 10,
                1, 2, 4, 7, 11), 
              2, 5, byrow = TRUE)

vec1 <- process_differences(smp, method = 1)
vec2 <- process_differences(smp, method = 2)
vec3 <- process_differences(smp, method = 3)

c(vec1)
c(vec2)
c(vec3)

```

## Thrash


Panel data structure $Y_{ij}$ for individual $i = 1, \ldots, n$ and occasion $j = 1, \ldots, l$.

# ECF


The empirical characteristic function of the random sample $\boldsymbol{X}_1, \ldots, \boldsymbol{X}_n$, where $\boldsymbol{X}_i$'s  are random vectors of size $d$ is defined as 
$$
\frac{1}{n}\sum_{j=1}^{n}\exp 
\left( \imath \langle \boldsymbol{X}_j, \boldsymbol{t}\rangle\right),
$$
where $\langle \cdot, \cdot \rangle$ is the usual inner product in $\mathbb{R}^d$. We present here functions do to find empirical characteristic function or its module and imaginary and real parts. 

Empirical characteristic functions are crucial for kernel deconvolution formulas and they are also used when the error distribution is not considered known. On the other hand, evaluate them can be time-consuming, thus, we have speeded up the evaluation time by implementing it in `C++` and allowing to compute only the part that we might be interested in, that is, the real or imaginary part, the modulus or the empirical characteristic function itself.

All the functions here described calculate empirical characteristic functions of samples from univariate or multivariate random variables.

## Computing empirical characteristic functions faster

Having a function that computes the empirical characteristic function (ecf) is practical, but if what the user needs is only the real part of the empirical characteristic function it is more efficient to do exactly that. The same is true for the modulus and the imaginary part. Taking the real part of the ecf takes twice the time it takes to compute directly the real part.

Therefore, functions have been implemented in `C++` for doing such operations directly, but only the wrappers in `R` are available to users.

Here it is an example of three ways to find the imaginary part of the ecf.

- ecf_im_cpp (way1): is the fastest, but it requires arguments to be
matrices with the appropriate arguments.

- ecf_imag (way2): is almost as fast as the function above, but it allows
vectors when it applies (either when the random sample is
unidimensional or the characteristic function is evaluated at only
one point).

- ecf_cpp (way3): takes twice the time, but it also computes the real
part.

```{r}
library(kerdec)

## Generate sample and define two points to evaluate the functions
smp <- rpois(5, lambda = 100)
t <- c(-1, 1)

## Evaluate the functions and arrange them in a dataframe.
dat <- 
  data.frame(
    way1 = kerdec:::ecf_im_cpp(matrix(t), matrix(smp)),
    way2 = ecf_imag(t, smp),
    way3 = Im(ecf(t, smp))
  )

## kable function is used for displaying the table nicely.
knitr::kable(dat)
```

## Univariate empirical characteristic function

This is a brief demonstration of the use of univariate functions for empirical characteristic functions. Observe that they all can be evaluated in vectors.

```{r, fig.height= 6, fig.width= 7}
library(kerdec)

## Parameters of Poisson distribution, sample size and grid size
lambda <- 10
n <- 150                                # Sample size
m <- 200                                # Grid size
t <- seq(-2, 2, length.out = m)         # Evaluation grid
smp <- rpois(n, lambda)                 # Random sample

## Compute empirical characteristic values and characteristic function
## values
real <- ecf_real(t, smp)
imag <- ecf_imag(t, smp)
modu <- ecf_mod(t, smp)
true <- exp(lambda*(exp(1i*t) - 1))

## Make plots
                                        # Real
plot(t, real, type = "l", col = 3)
lines(t, Re(true), col = 4)
title("Real part of empirical and true characteristic functions")
legend("topleft", legend = c("ecf", "cf"), col = 3:4, lwd = 2)

                                        # Imaginary
plot(t, imag, type = "l", col = 3)
lines(t, Im(true), col = 4)
title("Imaginary part of empirical and true characteristic functions")
legend("topleft", legend = c("ecf", "cf"), col = 3:4, lwd = 2)

                                        # Modulus
plot(t, modu, type = "l", col = 3, ylim = c(-0.05, 1))
lines(t, Mod(true), col = 4)
title("Modulus of empirical and true characteristic functions")
legend("topleft", legend = c("ecf", "cf"), col = 3:4, lwd = 2)

```

##  Empirical characteristic function of random vectors

The functions on this package work for ecf of random vectors. Since visualization of ecf of random vectors is not straightforward, we present an example of a random vector of size three and show a plot of how the absolute error (of the ecf evaluated in $10^3$ points) decreases as the sample size increases. 

It is worth mentioning that the ecf functions receive a vector of size $m$ or a $m \times d$ matrix as evaluation grid and a $n \times d$ as sample, where $m$ is the number of points where the ecf will be evaluated, $d$ the size of the random vector and $n$ the sample size.

```{r, fig.width= 7, fig.height= 6}
library(kerdec)

## Parameters of bivariate normal distribution
mu <- c(-1, 0, 1)
sig <- diag(1:3)

## Characteristic function
## s is n x d
phi <- function(s) {
    complex(modulus = exp(- 0.5*rowSums(s*(s %*% sig))),
            argument = s %*% mu)
}

## Random sample of dimension 3.
rndm <- function(n) {
    cbind(rnorm(n, mu[1], sig[1, 1]),
          rnorm(n, mu[2], sig[2, 2]),
          rnorm(n, mu[3], sig[3, 3]))
}

## Create evaluation grid.
grid_1d <- seq(-3, 3, length.out = 10)
grid <- as.matrix(expand.grid(t1 = grid_1d,
                              t2 = grid_1d,
                              t3 = grid_1d))

## Compute absolute error of modules
n <- seq(500, 5000, by = 500)
abs_error <- sapply(n, function(nn)
    sum(abs(Mod(phi(grid)) - ecf_mod(t = grid, smp = rndm(nn)))))

## Generate plot
plot(n, abs_error, type = "b", col = "magenta")

```



# Kernels

Kernel whose Fourier transform are preferred in kernel deconvolution methods due to their theoretical and numerical advantages. In fact, only those Fourier transforms are used, rather than the kernel functions themselves. Thus, we have a function that can calculates kernel and product kernels.

## Kernel functions and their Fourier transforms

The explicit expressions of kernel functions are not required in kernel deconvolution methods, but rather their Fourier transform. This package provides formulas for those Fourier transforms. We show below how they look like and then we show how do kernel functions look like, but we do use the package `fourierin` to obtain these values from their Fourier transforms.

The five kernel functions considered have boundedly supported Fourier transforms, specifically on $[-1, 1]$. We abuse on the nomenclature of the kernels, since "triangular", "tricube" and "triweight" do not correspond to the kernel functions with these names, but to the fact that their Fourier transform is proportional to these kernels. In the same manner, "flat-top" refers to a kernel whose Fourier transform is flat around zero.

```{r, fig.width= 7, fig.height= 5}
## Load package
library(kerdec)

## Define evaluation grid and kernels.
t <- seq(-1.3, 1.7, by = 0.01)
ker <- c("sinc", "triangular", "triweight", "tricube", "flat-top")

## Evaluate Ft of kernels.
values <- sapply(1:5, function(idx) ft_kernel(t, ker[idx]))

## Generate plots.
plot(t, rep(0, length(t)), type = "n", ylim = c(-0.01, 1),
     ylab = "value", main = "Fourier transform of kernels")
for(idx in 1:5){
  lines(t, values[, idx], col = idx + 1, lty = idx, lwd = 2)
}
legend("topright", ker, col = 2:6, lty = 1:5, lwd = 2)

```

Now we obtain the values of the kernel functions from their Fourier transforms.

```{r, fig.width= 7, fig.height= 5}
## Load package
library(kerdec)

## Write the name of all kernels available
ker <- c("sinc", "triangular", "triweight", "tricube", "flat-top")
names(ker) <- ker

## Find the actual kernel values from their Fourier transform
vals <- lapply(ker,
               function(krnl) 
                 fourierin::fourierin(function(t) ft_kernel(t, krnl), 
                                      a = -1, b = 1, c = -9, d = 9, 
                                      r = -1, s = -1, resol = 256))
## Obtain grid and extract the real part of every set of values.
x <- vals$sinc$w
vals <- lapply(vals, function(out) c(Re(out$values)))

## Generate plot
plot(x, vals[["sinc"]], type = "n", 
     ylab = "value", main = "Kernel functions")
abline(h = 0, col = "lightgray")
for(idx in 1:5){
  lines(x, vals[[idx]], col = idx + 1, lty = idx, lwd = 2)
}
legend("topright", ker, col = 2:6, lty = 1:5, lwd = 2)

```


## Univariate example

Although in the previous section we showed how to use the function that provides the Fourier transform of a particular set of kernel functions, we show a bare minimum example to illustrate how to use these function.

```{r, fig.width= 7, fig.height=5}
## Load package 
library(fourierin)

## Define grid
t <- seq(-1.3, 1.3, 0.01)

## Compute values
## We obtain the same result with ker = 2
values <- ft_kernel(t, ker = "triangular")

## Print plot
plot(t, values, main = "Triangular kernel", lwd = 2, 
     type = "l", col = "magenta")
```


## Product kernels

Finally we show a minimal example of how to use the function `ft_kernel` to find the Fourier transform of a product kernel. Observe that the grid has to be introduced as a matrix with $d$ columns and not as a $d$-dimensional array. 

```{r, fig.width= 7, fig.height=5}
## Load package 
library(fourierin)

## Define grid
t1 <- seq(-1.1, 1.1, length.out = 150)
t2 <- t1
t <- expand.grid(t1 = t1, t2 = t1)

## Compute values
## We obtain the same result with ker = 2
values <- ft_kernel(t, ker = "triangular")
values <- ft_kernel(t, ker = "flat-top")

## Convert dataframe to matrix
values <- matrix(values, length(t1))

## Print surface plot
persp(x = t1, y = t2, z = values,
      theta = 120, lwd = .2, phi = 30, shade = .8)

## Now a contour plot
contour(x = t1, y = t2, z = values, nlevels = 10)

```


# Thrash

ddd
